{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Understanding\n",
    "\n",
    "In this example, we will generate a description of a video using `Qwen2-VL`, `Qwen2-5-VL`, `LLava`, and `Idefics3`, with more models coming soon.\n",
    "\n",
    "This feature is currently in beta, may not work as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -U mlx-vlm\n",
    "\n",
    "```shell\n",
    "micromamba create -n mlx-vlm ipykernel python=3.12 -y\n",
    "micromamba activate mlx-vlm\n",
    "pip install mlx-vlm\n",
    "pip3 install torch torchvision torchaudio\n",
    "model, processor = load(\"mlx-community/Qwen2-VL-72B-Instruct-4bit\") # mlx-community/Qwen2-VL-72B-Instruct-4bit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lingyuzeng/.local/share/mamba/envs/mlx-vlm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "This is a beta version of the video understanding. It may not work as expected.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from mlx_vlm import load\n",
    "from mlx_vlm.utils import generate\n",
    "from mlx_vlm.video_generate import process_vision_info\n",
    "\n",
    "import mlx.core as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 18 files: 100%|██████████| 18/18 [00:00<00:00, 116149.96it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and processor\n",
    "# model, processor = load(\"mlx-community/Qwen2.5-VL-7B-Instruct-4bit\") # mlx-community/Qwen2-VL-72B-Instruct-4bit\n",
    "model, processor = load(\"mlx-community/Qwen2-VL-72B-Instruct-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "numpy reader: video_path=videos/fastmlx_local_ai_hub.mp4, total_frames=1134, video_fps=59.941855343141576, time=0.000s\n"
     ]
    }
   ],
   "source": [
    "# Messages containing a video and a text query\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"videos/fastmlx_local_ai_hub.mp4\",\n",
    "                \"max_pixels\": 360 * 360,\n",
    "                \"fps\": 1.0,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to mlx arrays\n",
    "input_ids = mx.array(inputs['input_ids'])\n",
    "pixel_values = mx.array(inputs['pixel_values_videos'])\n",
    "mask = mx.array(inputs['attention_mask'])\n",
    "image_grid_thw = mx.array(inputs['video_grid_thw'])\n",
    "\n",
    "kwargs = {\n",
    "    \"image_grid_thw\": image_grid_thw,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs[\"video\"] = \"videos/fastmlx_local_ai_hub.mp4\"\n",
    "kwargs[\"input_ids\"] = input_ids\n",
    "kwargs[\"pixel_values\"] = pixel_values\n",
    "kwargs[\"mask\"] = mask\n",
    "response = generate(model, processor, prompt=text, temp=0.7, max_tokens=100, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The video depicts a chaotic scene of numerous mobile phones floating in the '\n",
      " 'air, their screens displaying various data and information. The phones '\n",
      " 'appear to be in disarray, moving in different directions, creating a sense '\n",
      " 'of urgency and overwhelming information. The screens show a mix of text, '\n",
      " 'numbers, and graphs, suggesting a flood of data or notifications. The '\n",
      " 'background is dark, which makes the illuminated screens stand out even more, '\n",
      " 'adding to the overall impression of a digital overload.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdbb8704de74331827d4dc224e14ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00\\x18ftypisom\\x00\\x00\\x00\\x01isomiso4\\x00\\x00c<moov\\x00\\x00\\x00lmvhd...', height='240…"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open video and play it\n",
    "from ipywidgets import Video\n",
    "Video.from_file(\"videos/fastmlx_local_ai_hub.mp4\", width=320, height=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
