{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MLX-VLM MLX-VLM is a package for inference and fine-tuning of Vision Language Models (VLMs) on Apple silicon using MLX .","title":"Home"},{"location":"#mlx-vlm","text":"MLX-VLM is a package for inference and fine-tuning of Vision Language Models (VLMs) on Apple silicon using MLX .","title":"MLX-VLM"},{"location":"changelog/","text":"Changelog","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"cli_reference/","text":"CLI Reference MLX-VLM provides several command line entry points: mlx_vlm.convert \u2013 convert Hugging Face models to MLX format. mlx_vlm.generate \u2013 run inference on images. mlx_vlm.video_generate \u2013 generate from a video file. mlx_vlm.smolvlm_video_generate \u2013 lightweight video generation. mlx_vlm.chat_ui \u2013 start an interactive Gradio UI. mlx_vlm.server \u2013 run the FastAPI server. Each command accepts --help for full usage information.","title":"CLI Reference"},{"location":"cli_reference/#cli-reference","text":"MLX-VLM provides several command line entry points: mlx_vlm.convert \u2013 convert Hugging Face models to MLX format. mlx_vlm.generate \u2013 run inference on images. mlx_vlm.video_generate \u2013 generate from a video file. mlx_vlm.smolvlm_video_generate \u2013 lightweight video generation. mlx_vlm.chat_ui \u2013 start an interactive Gradio UI. mlx_vlm.server \u2013 run the FastAPI server. Each command accepts --help for full usage information.","title":"CLI Reference"},{"location":"community_projects/","text":"Community Projects If you have a project built on top of MLX-VLM let us know! We plan to showcase community examples and links here.","title":"Community Projects"},{"location":"community_projects/#community-projects","text":"If you have a project built on top of MLX-VLM let us know! We plan to showcase community examples and links here.","title":"Community Projects"},{"location":"contributing/","text":"Contributing To work on MLX-VLM in editable mode run: pip install -e . Check that the model weights are available in the safetensors format, convert if necessary and add the model file to mlx_vlm/models . Tests can be run from the mlx_vlm/ directory: python -m unittest discover tests/ Please format code using pre-commit before submitting a pull request.","title":"Contributing"},{"location":"contributing/#contributing","text":"To work on MLX-VLM in editable mode run: pip install -e . Check that the model weights are available in the safetensors format, convert if necessary and add the model file to mlx_vlm/models . Tests can be run from the mlx_vlm/ directory: python -m unittest discover tests/ Please format code using pre-commit before submitting a pull request.","title":"Contributing"},{"location":"examples/","text":"Examples Example notebooks are available in the examples/ directory: multi_image_generation.ipynb object_detection.ipynb object_pointing.ipynb ocr_with_region.ipynb text_extraction.ipynb video_understanding.ipynb Images and videos used by the notebooks are stored in the examples/images/ and examples/videos/ folders.","title":"Examples"},{"location":"examples/#examples","text":"Example notebooks are available in the examples/ directory: multi_image_generation.ipynb object_detection.ipynb object_pointing.ipynb ocr_with_region.ipynb text_extraction.ipynb video_understanding.ipynb Images and videos used by the notebooks are stored in the examples/images/ and examples/videos/ folders.","title":"Examples"},{"location":"installation/","text":"Installation Install the package from PyPI: pip install mlx-vlm","title":"Installation"},{"location":"installation/#installation","text":"Install the package from PyPI: pip install mlx-vlm","title":"Installation"},{"location":"report_issues/","text":"Report Issues Please open an issue on GitHub with clear steps to reproduce the problem.","title":"Report Issues"},{"location":"report_issues/#report-issues","text":"Please open an issue on GitHub with clear steps to reproduce the problem.","title":"Report Issues"},{"location":"usage/","text":"Usage Command Line Interface (CLI) Generate output from a model: python -m mlx_vlm.generate --model mlx-community/Qwen2-VL-2B-Instruct-4bit --max-tokens 100 --temperature 0.0 --image http://images.cocodataset.org/val2017/000000039769.jpg Chat UI with Gradio Launch the chat interface: python -m mlx_vlm.chat_ui --model mlx-community/Qwen2-VL-2B-Instruct-4bit Python Script from mlx_vlm import load, generate from mlx_vlm.prompt_utils import apply_chat_template from mlx_vlm.utils import load_config model_path = \"mlx-community/Qwen2-VL-2B-Instruct-4bit\" model, processor = load(model_path) config = load_config(model_path) image = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"] prompt = \"Describe this image.\" formatted_prompt = apply_chat_template(processor, config, prompt, num_images=len(image)) output = generate(model, processor, formatted_prompt, image, verbose=False) print(output) Server (FastAPI) python -m mlx_vlm.server See README.md for a complete curl example.","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#command-line-interface-cli","text":"Generate output from a model: python -m mlx_vlm.generate --model mlx-community/Qwen2-VL-2B-Instruct-4bit --max-tokens 100 --temperature 0.0 --image http://images.cocodataset.org/val2017/000000039769.jpg","title":"Command Line Interface (CLI)"},{"location":"usage/#chat-ui-with-gradio","text":"Launch the chat interface: python -m mlx_vlm.chat_ui --model mlx-community/Qwen2-VL-2B-Instruct-4bit","title":"Chat UI with Gradio"},{"location":"usage/#python-script","text":"from mlx_vlm import load, generate from mlx_vlm.prompt_utils import apply_chat_template from mlx_vlm.utils import load_config model_path = \"mlx-community/Qwen2-VL-2B-Instruct-4bit\" model, processor = load(model_path) config = load_config(model_path) image = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"] prompt = \"Describe this image.\" formatted_prompt = apply_chat_template(processor, config, prompt, num_images=len(image)) output = generate(model, processor, formatted_prompt, image, verbose=False) print(output)","title":"Python Script"},{"location":"usage/#server-fastapi","text":"python -m mlx_vlm.server See README.md for a complete curl example.","title":"Server (FastAPI)"}]}