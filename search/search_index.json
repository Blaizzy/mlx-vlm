{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLX-VLM","text":"<p>MLX-VLM is a package for inference and fine-tuning of Vision Language Models (VLMs) on Apple silicon using MLX.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"cli_reference/","title":"CLI Reference","text":"<p>MLX-VLM provides several command line entry points:</p> <ul> <li><code>mlx_vlm.convert</code> \u2013 convert Hugging Face models to MLX format.</li> <li><code>mlx_vlm.generate</code> \u2013 run inference on images.</li> <li><code>mlx_vlm.video_generate</code> \u2013 generate from a video file.</li> <li><code>mlx_vlm.smolvlm_video_generate</code> \u2013 lightweight video generation.</li> <li><code>mlx_vlm.chat_ui</code> \u2013 start an interactive Gradio UI.</li> <li><code>mlx_vlm.server</code> \u2013 run the FastAPI server.</li> </ul> <p>Each command accepts <code>--help</code> for full usage information.</p>"},{"location":"community_projects/","title":"Community Projects","text":"<p>If you have a project built on top of MLX-VLM let us know! We plan to showcase community examples and links here.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>To work on MLX-VLM in editable mode run:</p> <pre><code>pip install -e .\n</code></pre> <p>Check that the model weights are available in the <code>safetensors</code> format, convert if necessary and add the model file to <code>mlx_vlm/models</code>.</p> <p>Tests can be run from the <code>mlx_vlm/</code> directory:</p> <pre><code>python -m unittest discover tests/\n</code></pre> <p>Please format code using <code>pre-commit</code> before submitting a pull request.</p>"},{"location":"examples/","title":"Examples","text":"<p>Example notebooks are available in the <code>examples/</code> directory:</p> <ul> <li>multi_image_generation.ipynb</li> <li>object_detection.ipynb</li> <li>object_pointing.ipynb</li> <li>ocr_with_region.ipynb</li> <li>text_extraction.ipynb</li> <li>video_understanding.ipynb</li> </ul> <p>Images and videos used by the notebooks are stored in the <code>examples/images/</code> and <code>examples/videos/</code> folders.</p>"},{"location":"installation/","title":"Installation","text":"<p>Install the package from PyPI:</p> <pre><code>pip install mlx-vlm\n</code></pre>"},{"location":"report_issues/","title":"Report Issues","text":"<p>Please open an issue on GitHub with clear steps to reproduce the problem.</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#command-line-interface-cli","title":"Command Line Interface (CLI)","text":"<p>Generate output from a model:</p> <pre><code>python -m mlx_vlm.generate --model mlx-community/Qwen2-VL-2B-Instruct-4bit --max-tokens 100 --temperature 0.0 --image http://images.cocodataset.org/val2017/000000039769.jpg\n</code></pre>"},{"location":"usage/#chat-ui-with-gradio","title":"Chat UI with Gradio","text":"<p>Launch the chat interface:</p> <pre><code>python -m mlx_vlm.chat_ui --model mlx-community/Qwen2-VL-2B-Instruct-4bit\n</code></pre>"},{"location":"usage/#python-script","title":"Python Script","text":"<pre><code>from mlx_vlm import load, generate\nfrom mlx_vlm.prompt_utils import apply_chat_template\nfrom mlx_vlm.utils import load_config\n\nmodel_path = \"mlx-community/Qwen2-VL-2B-Instruct-4bit\"\nmodel, processor = load(model_path)\nconfig = load_config(model_path)\n\nimage = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"]\nprompt = \"Describe this image.\"\n\nformatted_prompt = apply_chat_template(processor, config, prompt, num_images=len(image))\noutput = generate(model, processor, formatted_prompt, image, verbose=False)\nprint(output)\n</code></pre>"},{"location":"usage/#server-fastapi","title":"Server (FastAPI)","text":"<pre><code>python -m mlx_vlm.server\n</code></pre> <p>See <code>README.md</code> for a complete <code>curl</code> example.</p>"}]}